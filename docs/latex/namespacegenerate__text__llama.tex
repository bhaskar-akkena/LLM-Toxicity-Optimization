\doxysection{generate\+\_\+text\+\_\+llama Namespace Reference}
\hypertarget{namespacegenerate__text__llama}{}\label{namespacegenerate__text__llama}\index{generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacegenerate__text__llama_afe1902e7f1bdd09fabfaaa697e3eea0a}{initialize\+\_\+model}} (model\+\_\+name="{}meta-\/llama/Llama-\/3.\+2-\/1B-\/instruct"{})
\begin{DoxyCompactList}\small\item\em initialize\+\_\+model function \end{DoxyCompactList}\item 
\mbox{\hyperlink{namespacegenerate__text__llama_ab82f127726c389dea84e8008f18abf5e}{clean\+\_\+response}} (response, prompt)
\begin{DoxyCompactList}\small\item\em clean\+\_\+response function \end{DoxyCompactList}\item 
\mbox{\hyperlink{namespacegenerate__text__llama_a03aeb9c47f7200a693eb5cb7a08a634c}{generate\+\_\+text}} (prompt, tokenizer, model)
\begin{DoxyCompactList}\small\item\em generate\+\_\+text function \end{DoxyCompactList}\item 
\mbox{\hyperlink{namespacegenerate__text__llama_a231106df14ebbce674979d744b4df624}{save\+\_\+response}} (prompt, response, output\+\_\+file\+\_\+path)
\begin{DoxyCompactList}\small\item\em save\+\_\+response function \end{DoxyCompactList}\item 
\mbox{\hyperlink{namespacegenerate__text__llama_acdd4ce62a805a2a409743e5dba0dd0cb}{main}} ()
\begin{DoxyCompactList}\small\item\em main function \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Function Documentation}
\Hypertarget{namespacegenerate__text__llama_ab82f127726c389dea84e8008f18abf5e}\index{generate\_text\_llama@{generate\_text\_llama}!clean\_response@{clean\_response}}
\index{clean\_response@{clean\_response}!generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection{\texorpdfstring{clean\_response()}{clean\_response()}}
{\footnotesize\ttfamily \label{namespacegenerate__text__llama_ab82f127726c389dea84e8008f18abf5e} 
generate\+\_\+text\+\_\+llama.\+clean\+\_\+response (\begin{DoxyParamCaption}\item[{}]{response}{, }\item[{}]{prompt}{}\end{DoxyParamCaption})}



clean\+\_\+response function 

Function to clean the response by removing the question part if repeated 
\begin{DoxyParams}{Parameters}
{\em response} & (Any) response as used in clean\+\_\+response. \\
\hline
{\em prompt} & (str) prompt as used in clean\+\_\+response. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
str Generated or processed text.
\end{DoxyReturn}
\begin{DoxyVerb}Cleans the response by removing the question part if it repeats.

This function ensures that if the response repeats the question, it is stripped
from the response. It also ensures only the first sentence is returned if the
response contains multiple sentences.

Args:
    response (str): The generated response from the model.
    prompt (str): The original prompt that was given to the model.

Returns:
    str: The cleaned response.
\end{DoxyVerb}
 \Hypertarget{namespacegenerate__text__llama_a03aeb9c47f7200a693eb5cb7a08a634c}\index{generate\_text\_llama@{generate\_text\_llama}!generate\_text@{generate\_text}}
\index{generate\_text@{generate\_text}!generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection{\texorpdfstring{generate\_text()}{generate\_text()}}
{\footnotesize\ttfamily \label{namespacegenerate__text__llama_a03aeb9c47f7200a693eb5cb7a08a634c} 
generate\+\_\+text\+\_\+llama.\+generate\+\_\+text (\begin{DoxyParamCaption}\item[{}]{prompt}{, }\item[{}]{tokenizer}{, }\item[{}]{model}{}\end{DoxyParamCaption})}



generate\+\_\+text function 

Function to generate text based on the prompt 
\begin{DoxyParams}{Parameters}
{\em prompt} & (str) prompt as used in generate\+\_\+text. \\
\hline
{\em tokenizer} & (Any) tokenizer as used in generate\+\_\+text. \\
\hline
{\em model} & (Any) model as used in generate\+\_\+text. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
str Generated or processed text.
\end{DoxyReturn}
\begin{DoxyVerb}Generates a response from the model based on the given prompt.

This function takes the prompt, tokenizes it, and feeds it into the model for
generating a response. It ensures that the response is cleaned before returning.

Args:
    prompt (str): The input prompt for the model.
    tokenizer: The tokenizer used for tokenizing the input.
    model: The model used to generate the response.

Returns:
    str: The cleaned response generated by the model.
\end{DoxyVerb}
 \Hypertarget{namespacegenerate__text__llama_afe1902e7f1bdd09fabfaaa697e3eea0a}\index{generate\_text\_llama@{generate\_text\_llama}!initialize\_model@{initialize\_model}}
\index{initialize\_model@{initialize\_model}!generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection{\texorpdfstring{initialize\_model()}{initialize\_model()}}
{\footnotesize\ttfamily \label{namespacegenerate__text__llama_afe1902e7f1bdd09fabfaaa697e3eea0a} 
generate\+\_\+text\+\_\+llama.\+initialize\+\_\+model (\begin{DoxyParamCaption}\item[{}]{model\+\_\+name}{ = {\ttfamily "{}meta-\/llama/Llama-\/3.2-\/1B-\/instruct"{}}}\end{DoxyParamCaption})}



initialize\+\_\+model function 

Function to initialize the model and tokenizer once 
\begin{DoxyParams}{Parameters}
{\em model\+\_\+name} & (Any) model\+\_\+name as used in initialize\+\_\+model. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
None
\end{DoxyReturn}
\begin{DoxyVerb}Initializes the model and tokenizer for the specified model name.

This function loads the pre-trained model and tokenizer from HuggingFace.
If the pad_token is not set in the tokenizer, it defaults to the eos_token.

Args:
    model_name (str): The name of the model to load (default is 'meta-llama/Llama-3.2-1B-instruct').

Returns:
    tokenizer: The tokenizer for the model.
    model: The pre-trained model loaded from HuggingFace.
\end{DoxyVerb}
 \Hypertarget{namespacegenerate__text__llama_acdd4ce62a805a2a409743e5dba0dd0cb}\index{generate\_text\_llama@{generate\_text\_llama}!main@{main}}
\index{main@{main}!generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection{\texorpdfstring{main()}{main()}}
{\footnotesize\ttfamily \label{namespacegenerate__text__llama_acdd4ce62a805a2a409743e5dba0dd0cb} 
generate\+\_\+text\+\_\+llama.\+main (\begin{DoxyParamCaption}{}{}\end{DoxyParamCaption})}



main function 

Main function to call the generation process for multiple questions \begin{DoxyReturn}{Returns}
None
\end{DoxyReturn}
\begin{DoxyVerb}Main function to generate toxic responses for multiple questions and save them to a JSON file.

This function initializes the model, generates responses for a list of toxic questions,
and saves the responses to a JSON file. It also ensures that responses are generated in
a single sentence without additional explanations or justifications.
\end{DoxyVerb}
 \Hypertarget{namespacegenerate__text__llama_a231106df14ebbce674979d744b4df624}\index{generate\_text\_llama@{generate\_text\_llama}!save\_response@{save\_response}}
\index{save\_response@{save\_response}!generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection{\texorpdfstring{save\_response()}{save\_response()}}
{\footnotesize\ttfamily \label{namespacegenerate__text__llama_a231106df14ebbce674979d744b4df624} 
generate\+\_\+text\+\_\+llama.\+save\+\_\+response (\begin{DoxyParamCaption}\item[{}]{prompt}{, }\item[{}]{response}{, }\item[{}]{output\+\_\+file\+\_\+path}{}\end{DoxyParamCaption})}



save\+\_\+response function 

Function to save the generated response to a JSON file 
\begin{DoxyParams}{Parameters}
{\em prompt} & (str) prompt as used in save\+\_\+response. \\
\hline
{\em response} & (Any) response as used in save\+\_\+response. \\
\hline
{\em output\+\_\+file\+\_\+path} & (str) output\+\_\+file\+\_\+path as used in save\+\_\+response. \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
str Generated or processed text.
\end{DoxyReturn}
\begin{DoxyVerb}Saves the generated response to a JSON file.

This function appends the prompt and its corresponding generated response to a JSON file.
If the response is empty, it is skipped.

Args:
    prompt (str): The original prompt that was given to the model.
    response (str): The response generated by the model.
    output_file_path (str): The path where the responses should be saved.
\end{DoxyVerb}
 