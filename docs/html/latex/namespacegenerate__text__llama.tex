\doxysection{generate\+\_\+text\+\_\+llama Namespace Reference}
\hypertarget{namespacegenerate__text__llama}{}\label{namespacegenerate__text__llama}\index{generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection*{Functions}
\begin{DoxyCompactItemize}
\item 
\mbox{\hyperlink{namespacegenerate__text__llama_afe1902e7f1bdd09fabfaaa697e3eea0a}{initialize\+\_\+model}} (model\+\_\+name="{}meta-\/llama/Llama-\/3.\+2-\/1B-\/instruct"{})
\item 
\mbox{\hyperlink{namespacegenerate__text__llama_ab82f127726c389dea84e8008f18abf5e}{clean\+\_\+response}} (response, prompt)
\item 
\mbox{\hyperlink{namespacegenerate__text__llama_a03aeb9c47f7200a693eb5cb7a08a634c}{generate\+\_\+text}} (prompt, tokenizer, model)
\item 
\mbox{\hyperlink{namespacegenerate__text__llama_a231106df14ebbce674979d744b4df624}{save\+\_\+response}} (prompt, response, output\+\_\+file\+\_\+path)
\item 
\mbox{\hyperlink{namespacegenerate__text__llama_acdd4ce62a805a2a409743e5dba0dd0cb}{main}} ()
\end{DoxyCompactItemize}


\doxysubsection{Function Documentation}
\Hypertarget{namespacegenerate__text__llama_ab82f127726c389dea84e8008f18abf5e}\index{generate\_text\_llama@{generate\_text\_llama}!clean\_response@{clean\_response}}
\index{clean\_response@{clean\_response}!generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection{\texorpdfstring{clean\_response()}{clean\_response()}}
{\footnotesize\ttfamily \label{namespacegenerate__text__llama_ab82f127726c389dea84e8008f18abf5e} 
generate\+\_\+text\+\_\+llama.\+clean\+\_\+response (\begin{DoxyParamCaption}\item[{}]{response}{, }\item[{}]{prompt}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Cleans the response by removing the question part if it repeats.

This function ensures that if the response repeats the question, it is stripped
from the response. It also ensures only the first sentence is returned if the
response contains multiple sentences.

Args:
    response (str): The generated response from the model.
    prompt (str): The original prompt that was given to the model.

Returns:
    str: The cleaned response.
\end{DoxyVerb}
 \Hypertarget{namespacegenerate__text__llama_a03aeb9c47f7200a693eb5cb7a08a634c}\index{generate\_text\_llama@{generate\_text\_llama}!generate\_text@{generate\_text}}
\index{generate\_text@{generate\_text}!generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection{\texorpdfstring{generate\_text()}{generate\_text()}}
{\footnotesize\ttfamily \label{namespacegenerate__text__llama_a03aeb9c47f7200a693eb5cb7a08a634c} 
generate\+\_\+text\+\_\+llama.\+generate\+\_\+text (\begin{DoxyParamCaption}\item[{}]{prompt}{, }\item[{}]{tokenizer}{, }\item[{}]{model}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Generates a response from the model based on the given prompt.

This function takes the prompt, tokenizes it, and feeds it into the model for
generating a response. It ensures that the response is cleaned before returning.

Args:
    prompt (str): The input prompt for the model.
    tokenizer: The tokenizer used for tokenizing the input.
    model: The model used to generate the response.

Returns:
    str: The cleaned response generated by the model.
\end{DoxyVerb}
 \Hypertarget{namespacegenerate__text__llama_afe1902e7f1bdd09fabfaaa697e3eea0a}\index{generate\_text\_llama@{generate\_text\_llama}!initialize\_model@{initialize\_model}}
\index{initialize\_model@{initialize\_model}!generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection{\texorpdfstring{initialize\_model()}{initialize\_model()}}
{\footnotesize\ttfamily \label{namespacegenerate__text__llama_afe1902e7f1bdd09fabfaaa697e3eea0a} 
generate\+\_\+text\+\_\+llama.\+initialize\+\_\+model (\begin{DoxyParamCaption}\item[{}]{model\+\_\+name}{ = {\ttfamily "{}meta-\/llama/Llama-\/3.2-\/1B-\/instruct"{}}}\end{DoxyParamCaption})}

\begin{DoxyVerb}Initializes the model and tokenizer for the specified model name.

This function loads the pre-trained model and tokenizer from HuggingFace.
If the pad_token is not set in the tokenizer, it defaults to the eos_token.

Args:
    model_name (str): The name of the model to load (default is 'meta-llama/Llama-3.2-1B-instruct').

Returns:
    tokenizer: The tokenizer for the model.
    model: The pre-trained model loaded from HuggingFace.
\end{DoxyVerb}
 \Hypertarget{namespacegenerate__text__llama_acdd4ce62a805a2a409743e5dba0dd0cb}\index{generate\_text\_llama@{generate\_text\_llama}!main@{main}}
\index{main@{main}!generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection{\texorpdfstring{main()}{main()}}
{\footnotesize\ttfamily \label{namespacegenerate__text__llama_acdd4ce62a805a2a409743e5dba0dd0cb} 
generate\+\_\+text\+\_\+llama.\+main (\begin{DoxyParamCaption}{}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Main function to generate toxic responses for multiple questions and save them to a JSON file.

This function initializes the model, generates responses for a list of toxic questions,
and saves the responses to a JSON file. It also ensures that responses are generated in
a single sentence without additional explanations or justifications.
\end{DoxyVerb}
 \Hypertarget{namespacegenerate__text__llama_a231106df14ebbce674979d744b4df624}\index{generate\_text\_llama@{generate\_text\_llama}!save\_response@{save\_response}}
\index{save\_response@{save\_response}!generate\_text\_llama@{generate\_text\_llama}}
\doxysubsubsection{\texorpdfstring{save\_response()}{save\_response()}}
{\footnotesize\ttfamily \label{namespacegenerate__text__llama_a231106df14ebbce674979d744b4df624} 
generate\+\_\+text\+\_\+llama.\+save\+\_\+response (\begin{DoxyParamCaption}\item[{}]{prompt}{, }\item[{}]{response}{, }\item[{}]{output\+\_\+file\+\_\+path}{}\end{DoxyParamCaption})}

\begin{DoxyVerb}Saves the generated response to a JSON file.

This function appends the prompt and its corresponding generated response to a JSON file.
If the response is empty, it is skipped.

Args:
    prompt (str): The original prompt that was given to the model.
    response (str): The response generated by the model.
    output_file_path (str): The path where the responses should be saved.
\end{DoxyVerb}
 